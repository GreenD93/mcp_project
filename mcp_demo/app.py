import streamlit as st
import requests
import json
from openai import OpenAI

openai_key = ""  # ì‹¤ì œ í‚¤ë¡œ êµì²´
client = OpenAI(api_key=openai_key)

MCP_SERVERS = {
    "weather": "http://localhost:8001",
    "news": "http://localhost:8002"
}

def fetch_tool_metadata():
    tools = []
    for name, base_url in MCP_SERVERS.items():
        try:
            res = requests.get(f"{base_url}/tools")
            for tool in res.json():
                tools.append({"mcp": name, "tool": tool})
        except Exception as e:
            st.error(f"[ERROR] {name} ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
    return tools

def ask_gpt_for_tool(user_input, tool_metadata):
    prompt = f"""
ì‚¬ìš©ì ì…ë ¥: "{user_input}"

ì•„ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ MCP íˆ´ ëª©ë¡ì…ë‹ˆë‹¤:
{json.dumps(tool_metadata, indent=2, ensure_ascii=False)}

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì— ì ì ˆí•œ MCP Toolì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ê³ , ìˆë‹¤ë©´ ì–´ë–¤ Toolì´ê³  ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ë„˜ê²¨ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì‘ë‹µ í˜•ì‹ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤:

1. ì‚¬ìš©ìì˜ ìš”ì²­ì— ì ì ˆí•œ Toolì´ ì¡´ì¬í•˜ê³ , íŒŒë¼ë¯¸í„°ë„ ì¶©ë¶„íˆ ì£¼ì–´ì§„ ê²½ìš°:
{{
  "mcp": "<mcp ì´ë¦„>",
  "tool_name": "<tool ì´ë¦„>",
  "arguments": {{ <íŒŒë¼ë¯¸í„° í‚¤:ê°’> }},
  "route": "TOOL"
}}

2. MCP Tool ëª©ë¡ ì¤‘ ì‚¬ìš©ì ìš”ì²­ì— ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” Toolì´ ì—†ê±°ë‚˜, í•„ìš”í•œ íŒŒë¼ë¯¸í„°ê°€ ë¶€ì¡±í•˜ì—¬ í˜¸ì¶œì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°:
{{
  "route": "DIRECT"
}}

ì‘ë‹µì€ ë°˜ë“œì‹œ ì½”ë“œ ë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSON í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”. (ì˜ˆ: ```json ê°™ì€ í¬ë§· ì—†ì´)
"""
    res = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
    )

    print('-'*50)
    print(prompt)
    print('-'*50)

    print('-'*50)
    print(res.choices[0].message.content)
    print('-'*50)

    return json.loads(res.choices[0].message.content)

def call_mcp(mcp_name, tool_name, args):
    try:
        url = f"{MCP_SERVERS[mcp_name]}/tool/{tool_name}"
        with requests.post(url, json=args, stream=True) as res:
            response_text = ""
            for chunk in res.iter_content(chunk_size=None):
                if chunk:
                    text = chunk.decode()
                    response_text += text
                    yield text
    except Exception as e:
        yield f"[ERROR] MCP í˜¸ì¶œ ì‹¤íŒ¨: {e}"

def direct_response(user_input):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ëŒ€ë‹µí•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤."},
            {"role": "user", "content": user_input}
        ],
        stream=True
    )
    for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# --- Streamlit UI ---
st.set_page_config(page_title="MCP ì±—ë´‡", layout="centered")
st.title("ğŸ¤– MCP ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "tool_metadata" not in st.session_state:
    st.session_state.tool_metadata = fetch_tool_metadata()

# ì´ì „ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ë Œë”ë§
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
if user_input:
    # íˆìŠ¤í† ë¦¬ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # ì±—ë´‡ ì‘ë‹µ ë Œë”ë§
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""

        decision = ask_gpt_for_tool(user_input, st.session_state.tool_metadata)

        if decision.get("route") == "DIRECT":
            for token in direct_response(user_input):
                full_response += token
                response_placeholder.markdown(full_response)
        else:
            mcp = decision["mcp"]
            tool = decision["tool_name"]
            args = decision["arguments"]
            for token in call_mcp(mcp, tool, args):
                full_response += token
                response_placeholder.markdown(full_response)

    # íˆìŠ¤í† ë¦¬ì— ì±—ë´‡ ì‘ë‹µ ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": full_response})
