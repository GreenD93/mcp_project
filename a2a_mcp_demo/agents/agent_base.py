import json
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Iterator

import requests
from openai import OpenAI

try:
    from jsonschema import Draft7Validator  # optional
except Exception:
    Draft7Validator = None


class MCPAgentBase:
    """
    ìµœì†Œ ì±…ì„:
      - tools/*/manifest.json + tools/mcp_servers.json ë¡œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ êµ¬ì„±
      - LLMìœ¼ë¡œ MCP ë„êµ¬ ì„ íƒ ì§ˆì˜ (ask_gpt_for_tool)
      - ì„ íƒëœ ë„êµ¬ í˜¸ì¶œ (call_mcp)
      - arguments JSON Schema ê²€ì¦ (validate_args)
    """

    init_system: str = ""

    def __init__(self, llm_client: OpenAI, agent_dir: Optional[Path] = None):
        self.llm: OpenAI = llm_client
        self.agent_dir: Path = agent_dir or Path(__file__).parent
        # ğŸ”¹ run ë³„ ëˆ„ì  ë¡œê·¸ ë²„í¼
        self.run_log: List[Dict[str, Any]] = []

        # card.json
        self.card: Dict[str, Any] = self._read_json(self.agent_dir / "card.json") or {}
        meta: Dict[str, Any] = self.card.get("metadata") or {}

        raw_tools = meta.get("tools", [])
        self.allow_all_tools: bool = False
        if isinstance(raw_tools, str) and raw_tools.strip() == "*":
            self.allow_all_tools = True
            self.allowed_servers: set[str] = set()
        elif isinstance(raw_tools, list):
            self.allowed_servers = set(raw_tools)
        else:
            self.allowed_servers = set()

        project_root = Path(__file__).resolve().parents[1]
        self.tools_root: Path = project_root / "tools"
        self.server_map: Dict[str, str] = self._read_json(self.tools_root / "mcp_servers.json") or {}
        self.registry: Dict[str, Dict[str, Dict[str, Any]]] = self._load_registry()

    # ---------------- Run-log helpers ----------------
    def reset_run_log(self):
        self.run_log = []

    def log(self, event: str, **fields):
        rec = {
            "ts": time.time(),
            "event": event,
            **fields
        }
        for k, v in list(rec.items()):
            if isinstance(v, str) and len(v) > 4000:
                rec[k] = v[:4000] + " â€¦(truncated)"
        self.run_log.append(rec)

    # ---------------- IO helpers ----------------
    def _read_json(self, path: Path) -> Optional[Dict[str, Any]]:
        if not path.exists():
            return None
        try:
            return json.loads(path.read_text(encoding="utf-8"))
        except Exception:
            return None

    def _load_registry(self) -> Dict[str, Dict[str, Dict[str, Any]]]:
        reg: Dict[str, Dict[str, Dict[str, Any]]] = {}
        if not self.tools_root.exists():
            return reg
        if not self.allow_all_tools and len(self.allowed_servers) == 0:
            return reg

        for server_dir in self.tools_root.iterdir():
            if not server_dir.is_dir():
                continue
            manifest = self._read_json(server_dir / "manifest.json")
            if not manifest:
                continue
            server = manifest.get("server")
            if not server:
                continue
            if not self.allow_all_tools and server not in self.allowed_servers:
                continue
            for t in manifest.get("tools", []):
                name = t.get("name")
                if not name:
                    continue
                reg.setdefault(server, {})[name] = {
                    "description": t.get("description", ""),
                    "parameters": t.get("parameters", {}) or {},
                    "path": t.get("path", f"/tool/{name}"),
                    "method": (t.get("method") or "POST").upper(),
                }
        return reg

    def list_tools_for_prompt(self) -> List[Dict[str, Any]]:
        out: List[Dict[str, Any]] = []
        for server, tools in self.registry.items():
            for name, spec in tools.items():
                out.append({
                    "mcp": server,
                    "tool_name": name,
                    "description": spec.get("description", ""),
                    "parameters": spec.get("parameters", {}),
                })
        return out

    def build_tool_selection_prompt(self, user_input: str) -> str:
        role_text = (
            self.init_system
            or (self.card.get("description") if isinstance(self.card, dict) else "")
            or "ë„êµ¬ë¥¼ ì ì ˆíˆ ì„ íƒí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì „ë¬¸ê°€"
        )
        tool_metadata = self.list_tools_for_prompt()

        prompt = f"""
ì—­í• : {role_text}

ì‚¬ìš©ì ì…ë ¥: "{user_input}"

ì•„ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ MCP íˆ´ ëª©ë¡ì…ë‹ˆë‹¤:
{json.dumps(tool_metadata, indent=2, ensure_ascii=False)}

ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì— ì ì ˆí•œ MCP Toolì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ê³ , ìˆë‹¤ë©´ ì–´ë–¤ Toolì´ê³  ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ë„˜ê²¨ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì„ ì •/ë¹„ì„ ì •ì˜ ì´ìœ (reason)ë¥¼ 1~2ë¬¸ì¥ìœ¼ë¡œ í•¨ê»˜ ì œê³µí•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ ê·œì¹™ (ì•„ì£¼ ì¤‘ìš”):
- ë°˜ë“œì‹œ ì•„ë˜ ì„¸ ê°€ì§€ í˜•ì‹ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
- JSONë§Œ ë‹¨ë…ìœ¼ë¡œ ì¶œë ¥í•´ì•¼ í•˜ë©°, ì–´ë– í•œ ì„¤ëª…, ì½”ë“œë¸”ë¡(ì˜ˆ: ```json), ì£¼ì„, ì¶”ê°€ í…ìŠ¤íŠ¸ë„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- JSON í‚¤ì™€ ê°’ì€ ì •í™•íˆ ì§€ì •ëœ êµ¬ì¡°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

1) í˜¸ì¶œ ê°€ëŠ¥ (í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì¶©ì¡± â†’ Tool ì‹¤í–‰ ê°€ëŠ¥)
{{
  "mcp": "<mcp ì´ë¦„>",
  "tool_name": "<tool ì´ë¦„>",
  "arguments": {{ <íŒŒë¼ë¯¸í„° í‚¤:ê°’> }},
  "route": "TOOL",
  "reason": "ì™œ ì´ ë„êµ¬ë¥¼ ì„ íƒí–ˆëŠ”ì§€ ê°„ë‹¨í•œ ê·¼ê±°"
}}

2) í˜¸ì¶œ ë¶ˆê°€ - Toolì€ ë§ì§€ë§Œ í•„ìˆ˜ íŒŒë¼ë¯¸í„° ë¶€ì¡±
{{
  "route": "TOOL_INCOMPLETE",
  "reason": "Toolì„ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ í•„ìˆ˜ íŒŒë¼ë¯¸í„°ê°€ ë¶€ì¡±í•˜ì—¬ í˜¸ì¶œ ë¶ˆê°€ëŠ¥í•œ ì´ìœ "
}}

3) í˜¸ì¶œ ë¶ˆê°€ - Toolì´ ì—†ê±°ë‚˜, ì—†ì–´ë„ ì§ì ‘ í•´ê²° ê°€ëŠ¥
{{
  "route": "DIRECT",
  "reason": "ì í•©í•œ Toolì´ ì—†ê±°ë‚˜, Toolì´ í•„ìš”í•˜ì§€ ì•Šì•„ ì§ì ‘ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì´ìœ "
}}

""".strip()
        self.log("tool.prompt", role_text=role_text, user_input=user_input, tool_count=len(tool_metadata))
        return prompt

    def ask_gpt_for_tool(self, user_input: str, *, prompt_override: Optional[str] = None) -> Dict[str, Any]:
        prompt = prompt_override or self.build_tool_selection_prompt(user_input)
        res = self.llm.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        raw = (res.choices[0].message.content or "").strip()
        self.log("tool.decision.raw", raw=raw)
        try:
            data = json.loads(raw)
        except Exception:
            self.log("tool.decision.parse_error")
            return {"route": "DIRECT", "error": "parse_error", "raw": raw}
        if "server" in data and "mcp" not in data:
            data["mcp"] = data.pop("server")
        if data.get("route") == "TOOL":
            if not data.get("mcp") or not data.get("tool_name"):
                return {"route": "DIRECT", "error": "missing_keys", "raw": data}
            data.setdefault("arguments", {})
        self.log("tool.decision.parsed", decision=data)
        return data

    def call_mcp(self, mcp: str, tool_name: str, args: Dict[str, Any], *, stream: bool = True):
        if mcp not in self.registry or tool_name not in self.registry[mcp]:
            raise RuntimeError(f"Unregistered tool: {mcp}.{tool_name}")
        if mcp not in self.server_map:
            raise RuntimeError(f"Unknown server host: {mcp}")

        spec = self.registry[mcp][tool_name]
        base = self.server_map[mcp].rstrip("/")
        url = f"{base}{spec['path']}"
        method = (spec["method"] or "POST").upper()

        t0 = time.time()
        self.log("mcp.call.start", mcp=mcp, tool=tool_name, url=url, method=method, args=args, stream=stream)

        if method == "GET":
            res = requests.get(url, params=args or {}, stream=stream, timeout=None)
        else:
            res = requests.post(url, json=args or {}, stream=stream, timeout=None)

        self.log("mcp.call.response.head",
                 status=res.status_code,
                 headers=dict(res.headers),
                 elapsed_ms=int((time.time() - t0) * 1000))
        res.raise_for_status()

        if not stream:
            data = res.json()
            try:
                preview = json.dumps(data, ensure_ascii=False)[:1000]
            except Exception:
                preview = str(data)[:1000]
            self.log("mcp.call.response.body", size=len(preview), preview=preview)
            return data

        def gen() -> Iterator[str]:
            bytes_total = 0
            for chunk in res.iter_content(chunk_size=None):
                if chunk:
                    bytes_total += len(chunk)
                    yield chunk.decode(errors="ignore")
            self.log("mcp.call.stream.end",
                     bytes_total=bytes_total,
                     elapsed_ms=int((time.time() - t0) * 1000))
        return gen()

    def execute(self, user_input: str, debug: Optional[Dict[str, Any]] = None):
        raise NotImplementedError

    # --------------- JSON Schema ê²€ì¦ ---------------
    def get_tool_schema(self, mcp: str, tool_name: str) -> Optional[Dict[str, Any]]:
        try:
            return self.registry[mcp][tool_name].get("parameters")
        except Exception:
            return None

    def validate_args(self, mcp: str, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë°˜í™˜: {"ok": bool, "errors": [str], "warnings": [str]}
        """
        result: Dict[str, Any] = {"ok": True, "errors": [], "warnings": []}
        schema = self.get_tool_schema(mcp, tool_name)
        if not schema:
            result["warnings"].append("no_schema: parameters schema not provided")
            return result

        # jsonschema ìˆìœ¼ë©´ í’€ ê²€ì¦
        if Draft7Validator is not None:
            try:
                validator = Draft7Validator(schema)
                errors = sorted(validator.iter_errors(arguments), key=lambda e: e.path)
                if errors:
                    result["ok"] = False
                    for e in errors:
                        loc = ".".join([str(p) for p in e.path]) or "(root)"
                        result["errors"].append(f"{loc}: {e.message}")
            except Exception as ex:
                result["ok"] = False
                result["errors"].append(f"validator_error: {ex}")
            return result

        # í´ë°±: í•„ìˆ˜/ê°„ë‹¨ íƒ€ì…ë§Œ
        try:
            req = list(schema.get("required") or [])
            props = dict(schema.get("properties") or {})
            for k in req:
                if k not in arguments:
                    result["ok"] = False
                    result["errors"].append(f"missing required property: '{k}'")

            _type_map = {
                "string": str, "number": (int, float), "integer": int,
                "boolean": bool, "object": dict, "array": list,
            }
            for k, v in arguments.items():
                if k in props:
                    t = props[k].get("type")
                    if t:
                        py = _type_map.get(t)
                        if py and not isinstance(v, py):
                            result["ok"] = False
                            result["errors"].append(
                                f"type mismatch at '{k}': expected {t}, got {type(v).__name__}"
                            )
            result["warnings"].append("fallback_validator: install 'jsonschema' for full validation")
        except Exception as ex:
            result["ok"] = False
            result["errors"].append(f"fallback_validator_error: {ex}")

        return result
    
    def _log(self, debug: Optional[Dict[str, Any]], event: str, **fields):
        """
        ê°„ë‹¨í•œ debug ë¡œê±°: debug ë”•ì…”ë„ˆë¦¬ì— events ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì´ë²¤íŠ¸ë¥¼ ì¶”ê°€
        """
        if debug is None:
            return
        try:
            debug.setdefault("events", []).append({"event": event, **fields})
        except Exception:
            pass

    def _incomplete_stream(self, user_input: str, reason: Optional[Dict[str, Any]] = None) -> Iterator[str]:

        user_prompt = (
            "ì‹¤íŒ¨ ì´ìœ ë¥¼ í† ëŒ€ë¡œ ì‚¬ìš©ìì—ê²Œ ì–‘í•´ë¥¼ êµ¬í•´ì¤˜.\n"
            "ì‚¬ìš©ìê°€ ì˜ ì´í•´í•  ìˆ˜ ìˆê²Œ ì¹œì ˆí•˜ê³  ì¤„ë°”ê¿ˆí•´ì„œ!\n"
            f"ì‚¬ìš©ì ìš”ì²­ : {user_input}"
            f"ì‹¤íŒ¨ ì´ìœ  : {reason}"
        )

        messages = [
            {"role": "user", "content": user_prompt},
        ]
        resp = self.llm.chat.completions.create(model="gpt-4o", messages=messages, stream=True)
        for ch in resp:
            if getattr(ch.choices[0].delta, "content", None):
                yield ch.choices[0].delta.content